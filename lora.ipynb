{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb062b3",
   "metadata": {},
   "source": [
    "pip install transformers datasets peft accelerate bitsandbytes\n",
    "cool link: https://towardsdatascience.com/lora-intuitively-and-exhaustively-explained-e944a6bff46b/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a68fe6",
   "metadata": {},
   "source": [
    "example json:\n",
    "[\n",
    "  { \"input\": \"Report 1: Crash on I-95. Report 2: Delays near downtown. Report 2: Crash on I-95. Report 2: Delays near downtown.\", \n",
    "    \"output\": \"I-95 crash and downtown congestion causing major delays.\" },\n",
    "\n",
    "  { \"input\": \"Report: Accident cleared on Route 1. Flow normal. Report 2: Crash on I-95. Report 2: Delays near downtown.\", \n",
    "    \"output\": \"Traffic back to normal on Route 1 after earlier accident.\" },\n",
    "\n",
    "  ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6955197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dostop do llame\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get token from environment\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Log in\n",
    "login(token=hf_token)\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81336868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === Config ===\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "dataset_path = \"your_dataset.json\"  # or Hugging Face dataset path\n",
    "output_dir = \"./llama3-lora-traffic\"\n",
    "\n",
    "# === Load tokenizer and model ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # needed for training\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# === Apply LoRA ===\n",
    "lora_config = LoraConfig(\n",
    "    r=8,            # the rank of the A and B matrices\n",
    "    lora_alpha=16,  # scaling factor, and by default it should be equal to r\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # which parts of the model LoRA should modify (the projection layers for query and value)\n",
    "    lora_dropout=0.1,       # hides inputs to prevent the model from overfitting\n",
    "    bias=\"none\",            # are we adding bias to the LoRA layers or just weights\n",
    "    task_type=TaskType.CAUSAL_LM,  # decoder-only LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# === Load dataset ===\n",
    "data = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "# === Preprocess --> converts raw input,output pairs into tokenized tensors ===\n",
    "# \"Summarize traffic news: report 1: Na dolenjski avtocesti zastoj... .Summary: <studentski report>\"\n",
    "def preprocess(example):\n",
    "    prompt = f\"Summarize traffic news:\\n{example['input']}\\nSummary:\"\n",
    "    output = example['output']\n",
    "    full_text = prompt + \" \" + output\n",
    "    return tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=512) # Truncates longer examples to fit max 512 tokens. Pads shorter ones to exactly 512 tokens.\n",
    "\n",
    "# Iterate over every example in dataset and apply the preprocess function\n",
    "tokenized_data = data.map(preprocess)\n",
    "\n",
    "# === Training setup ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,       # log loss every 10 steps\n",
    "    num_train_epochs=3,     # number of epochs to train, baje je 3 ze dost za loro \n",
    "    save_strategy=\"epoch\",  # save model every epoch\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,           # use 16-bit floating point precision\n",
    "    report_to=\"none\",    # disable logging to other services\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# === Train ===\n",
    "trainer.train()\n",
    "\n",
    "# === Save ===\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
